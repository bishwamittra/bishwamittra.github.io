\documentclass[10pt]{article}

% Define margins
\setlength{\topmargin}{-1.0cm}
\setlength{\oddsidemargin}{0.1cm}
\setlength{\textwidth}{16.5cm}
\setlength{\textheight}{23.0cm}

% Packages
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath}

% Color
\usepackage{xcolor}
\newcommand{\magenta}[1]{\textcolor{magenta}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}


% Long version
\newif\iflongproposal
\longproposaltrue % comment out for a short proposal



\begin{document}
	\begin{center}
		\large\textbf{Research Statement} 
	\end{center}
	\textbf{Bishwamittra Ghosh}\\		
	\noindent Scientist\\
	Institute of High Performance Computing (IHPC), A*STAR, Singapore\\
	\blue{\url{https://bishwamittra.github.io}}



	\paragraph{}
	My research is on fairness and explainability in machine learning applied in safety-critical domains. Traditional machine learning, particularly deep learning, is known for  unfair predictions towards marginalized sensitive groups and for generating black-box predictions. In my research, I design algorithmic frameworks to \textit{formally quantify fairness in machine learning}~\cite{ghosh2021justicia, ghosh2022algorithmic}, \textit{explain the sources of unfairness}~\cite{ghosh2022how}, and \textit{learn explainable rule-based classifiers}~\cite{ghosh22efficient, ghosh2019incremental, ghosh2020classification}. Prior approaches to these problems are often limited by scalability, accuracy, or both. To address the limitations, I closely integrate automated reasoning, formal methods, and statistics with fairness and explainability to develop scalable and accurate solutions.
	
	

	
	
	During my PhD, I have  research collaborations and internships in academia and industry. In addition to fairness and explainability, I have collaborative research on  group testing~\cite{ciampiconi2020maxsat}, social-spatial group queries~\cite{ghosh2018flexible, apon2021social}, and hypergraph core decomposition~\cite{arafat2023neighborhood}. We publish our research in leading conferences and journals in artificial intelligence and machine learning: AAAI ($2022$, $2021$, $2020$), JAIR ($2022$), FAccT ($2023$), ECAI ($2020$), and AIES ($2019$); and databases: VLDB ($2023$, $2018$) and TSAS ($2022$). I present a tutorial on Auditing Bias of Machine Learning Algorithms: Tools and Overview in IJCAI 2023. I was awarded the NUS Research Scholarship, Singapore and Moblilex Scholarship at Universit\'e de Lille, France.
	
	
	
	
	
	
%	\section*{Dissertation Research}
	
	\subsection*{Research Thrust 1: Fairness in Machine Learning}
	
	Fairness in machine learning focuses on quantifying and mitigating the  bias or unfairness of the prediction of the classifier towards different sensitive groups in the data. To quantify bias in algorithmic decision-making, multiple fairness metrics have been proposed based on societal norms and beliefs. However, there has been insignificant progress in \textit{formally quantifying existing fairness metrics}. In addition, fairness metrics measure the overall bias of a classifier, but they cannot  \textit{explain the sources of bias}. Therefore, our research focuses on two key aspects: formally quantifying bias of a classifier and explaining its sources.
	
	\subsubsection*{Probabilistic Fairness Quantification} In probabilistic fairness quantification, we formally quantify the bias of a classifier given the distribution of input features\textemdash essentially beyond a finite dataset. We propose two approaches to the problem: a general approach for finite classifiers encoded as Boolean formulas~\cite{ghosh2021justicia} and a specific approach for linear classifiers~\cite{ghosh2022algorithmic}.
	

	
	
	\paragraph{Fairness Quantification via SSAT.} The key idea in quantifying group fairness metrics is to compute the maximum (resp.\ minimum) probability of predictions of the classifier across all sensitive groups\textemdash the probability of selecting White-male vs.\ Black-female candidates in job applications. We propose a stochastic satisfiability (SSAT) based framework, called $\mathsf{Justicia}$~\cite{ghosh2021justicia}, for computing such probabilities. 
	\iflongproposal
	More specifically, the maximum probability becomes the solution of an existential-random (ER)-SSAT formula\textemdash we encode the classifier as a Boolean formula, the feature distribution via random Boolean variables, and compute the maximum conditional probability of the satisfaction of the formula for existentially quantified sensitive features.
	\fi
	In the presence of multiple sensitive features resulting in exponentially many sensitive groups, SSAT efficiently finds the most (resp.\ least) favored
	\iflongproposal
	group by the classifier, thanks to the progress in satisfiability (SAT) solving, and particularly in weighted model counting problem.
	\else
	group.
	\fi 
	In experiments, $\mathsf{Justicia}$ is more scalable in the fairness quantification of tree-based classifiers than existing SMT or sampling methods.
	
	
	\paragraph{Tractable Fairness Quantification with Feature Correlation.} We extend $\mathsf{Justicia}$ to consider feature correlations for an accurate fairness quantification~\cite{ghosh2022algorithmic}. We consider a Bayesian network to represent the conditional distribution of 
	\iflongproposal
	features\textemdash the SSAT formula grows with the complexity of the Bayesian network, calling for a more scalable solution.	Therefore, we
	\else
	features and
	\fi
	demonstrate a tractable fairness quantification for linear classifiers by proposing a stochastic subset sum 
	\iflongproposal
	problem, which admits an efficient dynamic programming solution with pseudo-polynomial complexity.
	\else
	problem.
	\fi 
	Experimentally, $\mathsf{Justicia}$ becomes more accurate and scalable than existing fairness verifiers for linear classifiers.
	
	
	
	
	
	\subsubsection*{Explaining Fairness: Identifying Sources of Bias}
	We combine both explainability and fairness in machine learning and propose a framework for explaining fairness.  We formalize \textit{fairness influence functions} (FIFs) to quantify the contribution of an individual feature and the intersection of multiple features to the resulting bias of the classifier~\cite{ghosh2022how}. 	Based on global sensitivity analysis, we propose a model-agnostic framework, called $\mathsf{FairXplainer}$, to estimate FIFs. 
	\iflongproposal
	The key idea is to represent fairness metrics using the variance of predictions and apply variance decomposition to compute FIFs.
	\fi
	In experiments, FIFs are highly correlated with fairness  
	\iflongproposal
	interventions and demonstrate a higher granular explanation of unfairness through intersectional influences,
	\else 
	interventions, 
	\fi
	unlike existing local explainability methods. In addition, $\mathsf{FairXplainer}$ approximates bias via FIFs with lower error than prior 
	\iflongproposal
	methods across classifiers such as neural networks and SVMs.
	\else
	methods.
	\fi
	
	

		
	
	\subsection*{Research Thrust 2: Explainable Rule-based Machine Learning}

	We learn classifiers explainable by design, such as  rule-based 
	\iflongproposal
	classifiers. In rule-based classifiers, for example decision lists and decision sets, the decision boundary is explained using a set of rules relating input features to class prediction.
	\else
	classifiers.
	\fi
	The explainability of such classifiers often depends on the size of the rules\textemdash smaller rules with higher accuracy are preferred in \iflongproposal
	practice, particularly by practitioners in the medical domain.
	\else
	practice.
	\fi 
	Our contributions in rule-based classification are two-folds: a scalable learning  framework for classification rules by incremental learning and an improvement of the expressiveness of rules via logical relaxation.
	
	\subsubsection*{Scalability via Incremental Learning}
	We introduce an incremental learning framework based on MaxSAT, called $\mathsf{IMLI}$~\cite{ghosh22efficient,ghosh2019incremental}, to learn explainable classification rules in propositional logic, particularly in CNF. The CNF learning framework can potentially learn other explainable representations: decision sets, decision lists etc.
	We design a MaxSAT formulation to jointly optimize the accuracy and explainability of CNF 
	\iflongproposal
	classifiers, and leverage the progress in MaxSAT solving to efficiently learn an optimal classifier. 
	\else
	classifiers. 
	\fi 
	However, the MaxSAT formula grows with dataset dimension and classifier size. To improve scalability, $\mathsf{IMLI}$ integrates both mini-batch learning and iterative 
	\iflongproposal
	rule-learning: $\mathsf{IMLI}$ learns a CNF classifier by iteratively covering the training data, where in each iteration $\mathsf{IMLI}$ solves a sequence of smaller MaxSAT queries respective to mini-batches. 
	\else
	rule-learning with MaxSAT solving.
	\fi
	In experiments, $\mathsf{IMLI}$ achieves the best balance among prediction accuracy, explainability, and scalability, for example, a competitive accuracy and explainability compared to existing rule-based classifiers, and a higher scalability on large datasets with a million samples where  explainable and non-explainable classifiers may fail. 
	
	\subsubsection*{Expressiveness via Logical Relaxation}
	Rule-based classifiers are explainable by design, but they are less expressive. We propose a more expressible yet explainable rule-based classifier, called relaxed-CNF~\cite{ghosh2020classification}, based on a relaxed definition of the standard OR/AND operators in logic. \iflongproposal
	Motivated by checklists in the medical domain such as $ \text{CHADS}_2 $ score, in relaxed-CNF, both the minimum number of literals satisfied in a clause and the maximum number of clauses satisfied in a formula are flexible. As a result, relaxed-CNF
	\else
	Relaxed-CNF 
	\fi
	 generalizes widely used rule representations: CNF, DNF, decision lists, and decision sets. While the combinatorial structure of relaxed-CNF results in exponential succinctness, the direct learning technique is computationally expensive. Therefore, we extend $\mathsf{IMLI}$ and propose an incremental mini-batch learning procedure for relaxd-CNF classifiers, called $\mathsf{CRR}$, by leveraging advances in MILP solving. In experiments, $\mathsf{CRR}$ generates more accurate yet smaller relaxed-CNF rules compared to alternative rule-based classifiers.
	
	
	

	
	
	
	

	\section*{Future Research Plans}
	
	
	
	 My future research is dedicated to developing practical and scalable algorithms for trustworthy machine learning. Machine learning and artificial intelligence have been compared to the new electricity, with the potential to transform various aspects of human life, evident from the overwhelming response to generative AI. Ensuring fairness and explainability in deployed machine learning is now more necessary than ever. To accomplish this, I aim to work in a collaborative environment, gaining insights into real-world challenges and leveraging advances in the field, alongside formal methods, to make significant progress. I have identified key research themes that will guide my work towards this vision.
	 
	 
	 	\paragraph{Fairness and Explainability As a Service.} 
	 	The goal of modern machine learning extends beyond learning patterns from large-scale historical data to ensuring responsible decision-making through careful regulation to establish trustworthiness. For instance, in a job application scenario, a machine learning algorithm must be fair across different demographic groups, resilient to non-actionable changes in candidate profiles, and explainable to allow candidates to understand the decision-making process. My long-term research goal is to offer fairness and explainability as a service with machine learning-based decision-making. Below, I outline several research ideas concerning fairness and explainability:
	 	
	 	\begin{itemize}
	 		\item \textbf{Fairness Auditing.}
	 		Our objective is to develop a comprehensive fairness auditing framework for machine learning, focusing on three key questions. (i) \textit{Which fairness metrics to choose?} We aim to identify the most appropriate fairness metrics for specific application contexts, as choosing the right metric is crucial among various notions of fairness. (ii) \textit{How to quantify bias?} We extend formal fairness quantification to encompass broader fairness metrics, including individual fairness, causal fairness, and counterfactual fairness. We apply this extension to
	 		 unstructured data such as images and texts, and classifiers such as random forests, neural networks, and language models.  (iii) \textit{How to explain bias?}  We strive to advance our $\mathsf{FairXplainer}$ to explain bias for both texts and images. For instance, in conversational AI, we intend to highlight the input prompts that trigger biased statements generation by the model. By addressing these three questions, our vision is to design improved bias mitigating algorithms with significant practical impacts.
	 		 
	 		 \item \textbf{Explainability with Guarantees.} Our research in explainable machine learning spans two main directions.  (i) \textit{Explainability by design:} There is a growing interest for explainable machine learning in safety-critical domains, for example, clinical predictions, financial decisions, and self-driving vehicles. Building upon our explainable rule-based classifier $ \mathsf{IMLI} $, we aim to enhance learning algorithms for explainable models in large-scale datasets across supervised, semi-supervised, and unsupervised settings. (ii) \textit{Post-hoc explainability:} To explain black-box predictions, we focus on explanations with formal guarantees. For example, an explanation model must be robust, learned in a privacy-preserving manner, and provide the confidence level of explanations to increase transparency and trust in the decision-making process.
	 	\end{itemize}
	 	
	 	
	 
		\paragraph{Verifiable Machine Learning with Formal Methods.} In safety-critical and high-stake domains, the verification of machine learning before deployment is crucial. To this end, formal methods provide a template to verify different checkable properties of machine learning. In particular, SAT, SMT and their variants allow to concentrate on synthesizing constraints from real-world use cases and delegate the solution finding to respective solvers, thanks to the dedicated community in formal methods. Building upon my ongoing research, which involves applying SSAT and MaxSAT to address fairness and explainability challenges, I plan to explore additional formulations in formal methods, such as functional analysis, abstract interpretation, and solvers with expressive theory, to further enhance the verification of machine learning models.
	
	
 
	
	
	
	\paragraph{Benchmarking Machine Learning: Improvement in Formal Methods.} Interdisciplinary research is an ongoing process that yields valuable contributions to different disciplines. Our work on fairness and explainability in machine learning has had a positive impact on the improvement of solvers in formal methods. For instance, when tackling explainable classification problems, we realized that MaxSAT alone was insufficient for large-scale rule-based classification, prompting the need for an incremental MaxSAT solver. As a result, we contribute to the MaxSAT evaluation competition in 2019 with our MaxSAT benchmarks of explainable classification. Subsequently, the competition introduced an incremental MaxSAT solving track. This observation serves as strong motivation for me to continue contributing to the formal methods community by creating additional machine learning verification benchmarks.
		 
	 
	
	\small
	\bibliographystyle{ieeetr}
	\bibliography{ref}
\end{document}